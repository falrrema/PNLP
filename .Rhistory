installThesePackages <- function() {
list.of.packages <- c("magrittr", "ggthemes","extrafont", "googlesheets","tidyr",
"googlesheets", "lubridate", "magrittr", "ggplot2", "plotly", "dplyr",
"data.table", "tm", "readxl", "RColorBrewer", "ngram", "wordcloud",
"httpuv", "tm", "text2vec", "wordcloud", "parallel", "gofastr",
"pacman", "scales", "tidytext", "mlr", "topicmodels", "SnowballC")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
}
installThesePackages()
word_tokenizer
#################
# Preprocessing
#################
setwd("~/Kaggle/PNLP")
Sys.setlocale(locale = "es_ES.UTF-8") # Para visualizar caracteres especiales
library(text2vec)
library(data.table)
library(dplyr)
library(magrittr)
source("keyFunctions.R")
what <- "train"
# Leyendo datos y transformando
if (what == "train") {
df <- fread("data/train_features.csv")
} else if (what == "test") {
df <- fread("data/test_features.csv")
}
setwd("~/Google Drive/PNLP")
Sys.setlocale(locale = "es_ES.UTF-8") # Para visualizar caracteres especiales
library(text2vec)
library(data.table)
library(dplyr)
library(magrittr)
source("keyFunctions.R")
what <- "train"
if (what == "train") {
df <- fread("data/train_features.csv")
} else if (what == "test") {
df <- fread("data/test_features.csv")
}
start.time <- Sys.time()
View(df)
#########################
# Key functions for PNLP
########################
if (!require("tm")) install.packages("tm"); library(tm)
if (!require("SnowballC")) install.packages("SnowballC"); library(SnowballC)
if (!require("parallel")) install.packages("parallel"); library(parallel)
# CleanText ---------------------------------------------------------------
# "removeMostPunctuation" is the function that allows flexible punctuation removal. Those puntuation marks you want to preserve
# are needed to be pass as a vector, for example, c("@", "#"). By default this value is NULL, which the functions then
# goes back to base "removePunctuation()".
removeMostPunctuation <- function (text, preserveWhich = NULL) { # The functionality is base in tagging, removing and detagging.
if (!is.null(preserveWhich)) {
for (i in 1:length(preserveWhich)) {
replacement <- paste("000", i, sep = "")
text <- gsub(preserveWhich[i], replacement, text)
}
text <- removePunctuation(text)
for (i in 1:length(preserveWhich)) {
replacement <- paste("000", i, sep = "")
text <- gsub(replacement, preserveWhich[i], text)
}
} else {
text <- removePunctuation(text)
}
return(text)
}
#################
# Preprocessing
#################
setwd("~/Google Drive/PNLP")
Sys.setlocale(locale = "es_ES.UTF-8") # Para visualizar caracteres especiales
library(text2vec)
library(data.table)
library(dplyr)
library(magrittr)
source("keyFunctions.R")
what <- "train"
# Leyendo datos y transformando
if (what == "train") {
df <- fread("data/train_features.csv")
} else if (what == "test") {
df <- fread("data/test_features.csv")
}
#################
# Preprocessing
#################
setwd("~/Google Drive/PNLP")
Sys.setlocale(locale = "es_ES.UTF-8") # Para visualizar caracteres especiales
library(text2vec)
library(data.table)
library(dplyr)
library(magrittr)
source("keyFunctions.R")
what <- "train"
# Leyendo datos y transformando
if (what == "train") {
df <- fread("data/train_features.csv")
} else if (what == "test") {
df <- fread("data/test_features.csv")
}
#################
# Preprocessing
#################
setwd("~/Google Drive/PNLP")
Sys.setlocale(locale = "es_ES.UTF-8") # Para visualizar caracteres especiales
library(text2vec)
library(data.table)
library(dplyr)
library(magrittr)
source("keyFunctions.R")
what <- "train"
# Leyendo datos y transformando
if (what == "train") {
df <- fread("data/train_features.csv")
} else if (what == "test") {
df <- fread("data/test_features.csv")
}
start.time <- Sys.time()
df <- getSizeFeatures(df, question1, question2) # Variables básica de diffLargo
df$wordShare <- wordShareIndex(df, question1, question2) # Variable de % de palabras compartidas
df <- getDistFeatures(df, question1, question2) # Variables de distancia de documentos
df <- getGloveFeature(df, question1, question2)
end.time <- Sys.time()
cat("Tiempo estimado de ejecución:", difftime(end.time, start.time, units = c("hours")))
View(df)
source("keyFunctions.R")
getWordVectors
df <- getGloveFeature(df, question1, question2)
View(df)
n_cores <- detectCores() - 2 # Calculate the number of cores
n_cores
detectCores()
source("keyFunctions.R")
df <- getGloveFeature(df, question1, question2)
if (!require("pbapply")) install.packages("pbapply"); library(parallel)
if (!require("pbapply")) install.packages("pbapply"); library(pbapply)
df <- getGloveFeature(df, question1, question2)
View(df)
#################
# Preprocessing
#################
setwd("~/Google Drive/PNLP")
Sys.setlocale(locale = "es_ES.UTF-8") # Para visualizar caracteres especiales
library(text2vec)
library(data.table)
library(dplyr)
library(magrittr)
source("keyFunctions.R")
what <- "train"
# Leyendo datos y transformando
if (what == "train") {
df <- fread("data/train_features.csv")
} else if (what == "test") {
df <- fread("data/test_features.csv")
}
View(df)
# Feature extraction ----------------------------------------------------
start.time <- Sys.time()
df <- getSizeFeatures(df, question1, question2) # Variables básica de diffLargo
df$wordShare <- wordShareIndex(df, question1, question2) # Variable de % de palabras compartidas
getSizeFeatures
View(df)
data <- df[1]
data <- df[1:1000, ]
.string1 <- col_name(substitute(question1))
.string2 <- col_name(substitute(question2))
stopW <- prep_fun(tm::stopwords("en"))
data <- data.table(data)
cat("Cleaning Stopwords", "\n")
dt <- tidyr::gather(data, preg, text, get(.string1), get(.string2))
View(dt)
View(data)
data %>% select(id, get(.string1), get(.string2))
dt <- data %>% select(id, get(.string1), get(.string2)) %>%
tidyr::gather(preg, text, get(.string1), get(.string2))
View(dt)
nCores = 0
n_cores <- detectCores() - nCores # Calculate the number of cores
core_clusters <- makeCluster(n_cores, type = "FORK") # Initiate cluster
dt$textClean <- pbapply::pbsapply(dt$text, function(t) {
clean <- prep_fun(t, stopW) # Sin stopwords
return(clean)
}, cl = core_clusters)
stopCluster(core_clusters) # End cluster usage
View(dt)
dt %>%
select(-text)
dt <- dt %>%
select(-text) %>%
tidyr::spread(preg, textClean) %>%
rename(question1Clean = question1, question2Clean = question2)
View(dt)
wordCount <- function(text) {
stringr::str_count(text, pattern = "\\S+") # conteo palabras
}
cat("Calculating word difference", "\n")
wordDiff <- dt %>% select(starts_with("question")) %>%
mutate_each(funs(wordCount)) %>%
transmute(word_diff = abs(question1-question2), word_diff_stop = abs(question1Clean-question2Clean))
dt %>% select(starts_with("question"))
dt %>% select(starts_with("question")) %>%
mutate_each(funs(wordCount))
dt <- data %>% select(id, get(.string1), get(.string2)) %>%
tidyr::gather(preg, text, get(.string1), get(.string2))
n_cores <- detectCores() - nCores # Calculate the number of cores
core_clusters <- makeCluster(n_cores, type = "FORK") # Initiate cluster
dt$textClean <- pbapply::pbsapply(dt$text, function(t) {
clean <- prep_fun(t, stopW) # Sin stopwords
return(clean)
}, cl = core_clusters)
stopCluster(core_clusters) # End cluster usage
dt <- dt %>%
select(-text) %>%
tidyr::spread(preg, textClean) %>%
rename(question1Clean = question1, question2Clean = question2)
View(dt)
data %>%
select(id, get(.string1), get(.string2)) %>% names
dt <- data %>% select(id, get(.string1), get(.string2)) %>%
tidyr::gather(preg, text, get(.string1), get(.string2))
n_cores <- detectCores() - nCores # Calculate the number of cores
core_clusters <- makeCluster(n_cores, type = "FORK") # Initiate cluster
dt$textClean <- pbapply::pbsapply(dt$text, function(t) {
clean <- prep_fun(t, stopW) # Sin stopwords
return(clean)
}, cl = core_clusters)
stopCluster(core_clusters) # End cluster usage
dt <- dt %>%
select(-text) %>%
tidyr::spread(preg, textClean) %>%
rename(question1Clean = get(.string1), question2Clean = get(.string2))
View(dt)
dt <- dt %>%
select(-text) %>%
tidyr::spread(preg, textClean)
View(dt)
?rename
dt <- dt %>%
select(-text) %>%
tidyr::spread(preg, textClean) %>%
rename_(question1Clean = get(.string1), question2Clean = get(.string2))
dt <- data %>% select(id, get(.string1), get(.string2)) %>%
tidyr::gather(preg, text, get(.string1), get(.string2))
n_cores <- detectCores() - nCores # Calculate the number of cores
core_clusters <- makeCluster(n_cores, type = "FORK") # Initiate cluster
dt$textClean <- pbapply::pbsapply(dt$text, function(t) {
clean <- prep_fun(t, stopW) # Sin stopwords
return(clean)
}, cl = core_clusters)
stopCluster(core_clusters) # End cluster usage
dt <- dt %>%
select(-text) %>%
tidyr::spread(preg, textClean) %>%
rename_(question1Clean = get(.string1), question2Clean = get(.string2))
get(.string1)
dt <- dt %>%
select(-text) %>%
tidyr::spread(preg, textClean) %>%
rename_(question1Clean = .string1, question2Clean = .string2)
View(dt)
dt <- data %>% select_(id, .string1, .string2) %>%
tidyr::gather(preg, text, get(.string1), get(.string2))
cat("Cleaning Stopwords", "\n")
dt <- data %>% select_(id, .string1, .string2) %>%
tidyr::gather(preg, text, get(.string1), get(.string2))
.string2
dt <- data %>% select_("id", .string1, .string2) %>%
tidyr::gather(preg, text, get(.string1), get(.string2))
n_cores <- detectCores() - nCores # Calculate the number of cores
core_clusters <- makeCluster(n_cores, type = "FORK") # Initiate cluster
dt$textClean <- pbapply::pbsapply(dt$text, function(t) {
clean <- prep_fun(t, stopW) # Sin stopwords
return(clean)
}, cl = core_clusters)
stopCluster(core_clusters) # End cluster usage
.id <- col_name(substitute(id))
.id <- col_name(substitute(id))
cat("Cleaning Stopwords", "\n")
dt <- data %>% select_(.id, .string1, .string2) %>%
tidyr::gather(preg, text, get(.string1), get(.string2))
n_cores <- detectCores() - nCores # Calculate the number of cores
core_clusters <- makeCluster(n_cores, type = "FORK") # Initiate cluster
dt$textClean <- pbapply::pbsapply(dt$text, function(t) {
clean <- prep_fun(t, stopW) # Sin stopwords
return(clean)
}, cl = core_clusters)
stopCluster(core_clusters) # End cluster usage
dt <- dt %>%
select(-text) %>%
tidyr::spread(preg, textClean) %>%
rename_(question1Clean = .string1, question2Clean = .string2)
dt <- data %>%
select_(.id, .string1, .string2) %>%
inner_join(dt, by = .id)
View(dt)
wordCount <- function(text) {
stringr::str_count(text, pattern = "\\S+") # conteo palabras
}
dt %>% select_(-.id)
dt %>% select_(-(.id))
dt %>% select_(quote(-id)) %>% names
wordDiff <- dt %>% select_(quote(-id)) %>%
mutate_each(funs(wordCount)) %>%
transmute_(word_diff = abs(.string1-.string2), word_diff_stop = abs("question1Clean"-"question2Clean"))
?transmute
wordDiff <- dt %>% select_(quote(-id)) %>%
mutate_each(funs(wordCount)) %>%
transmute_(word_diff = abs(quote(string1)-quote(string2)), word_diff_stop = abs(quote(question1Clean)-quote(question2Clean)))
wordDiff <- dt %>% select_(quote(-id)) %>%
mutate_each(funs(wordCount)) %>%
transmute(word_diff = abs(get(.string1) - get(.string2)), word_diff_stop = abs(question1Clean - question2Clean))
View(dt)
get(.string1)
?setNames
mutate_call1 <- lazyeval::interp(~ abs(a - b), a = as.name(.string1), b = as.name(.string2))
as.name("question1Clean")
mutate_call2 <- lazyeval::interp(~ abs(a - b), a = as.name("question1Clean"), b = as.name("question2Clean"))
wordDiff <- dt %>% select_(quote(-id)) %>%
mutate_each(funs(wordCount)) %>%
transmute(.dots = setNames(list(mutate_call1), word_diff),
.dots = setNames(list(mutate_call2), word_diff_stop))
mutate_call1 <- lazyeval::interp(~ abs(a - b), a = as.name(.string1), b = as.name(.string2))
mutate_call2 <- lazyeval::interp(~ abs(a - b), a = as.name("question1Clean"), b = as.name("question2Clean"))
wordDiff <- dt %>% select_(quote(-id)) %>%
mutate_each(funs(wordCount)) %>%
transmute(.dots = setNames(list(mutate_call1), word_diff),
.dots = setNames(list(mutate_call2), word_diff_stop))
wordDiff <- dt %>% select_(quote(-id)) %>%
mutate_each(funs(wordCount)) %>%
transmute(.dots = setNames(list(mutate_call1), "word_diff"),
.dots = setNames(list(mutate_call2), "word_diff_stop"))
wordDiff <- dt %>% select_(quote(-id)) %>%
mutate_each(funs(wordCount)) %>%
transmute_(.dots = setNames(list(mutate_call1), "word_diff"),
.dots = setNames(list(mutate_call2), "word_diff_stop"))
wordDiff <- dt %>% select_(quote(-id)) %>%
mutate_each(funs(wordCount)) %>%
transmute_(.dots = setNames(list(mutate_call1), "word_diff"),
.dots2 = setNames(list(mutate_call2), "word_diff_stop"))
list(mutate_call1)
setNames(list(mutate_call1), "word_diff")
setNames(list(mutate_call2), "word_diff_stop")
dt %>% select_(quote(-id)) %>%
mutate_each(funs(wordCount))
wordDiff <- dt %>% select_(quote(-id)) %>%
mutate_each(funs(wordCount)) %>%
transmute_(.dots = setNames(list(mutate_call1), "word_diff"),
.dots2 = setNames(list(mutate_call2), "word_diff_stop"))
wordDiff <- dt %>% select_(quote(-id)) %>%
mutate_each(funs(wordCount)) %>%
transmute_(.dots = setNames(mutate_call1, "word_diff"),
.dots2 = setNames(mutate_call2, "word_diff_stop"))
View(wordDiff)
wordDiff <- dt %>% select_(quote(-id)) %>%
mutate_each(funs(wordCount)) %>%
transmute_(word_diff = setNames(mutate_call1),
word_diff_stop = setNames(mutate_call2))
setNames()
?setNames()
wordDiff <- dt %>% select_(quote(-id)) %>%
mutate_each(funs(wordCount)) %>%
transmute_(word_diff = setNames(mutate_call1, "word_diff"),
word_diff_stop = setNames(mutate_call2, "word_diff_stop"))
View(wordDiff)
wordDiff2 <- dt %>% select_(quote(-id)) %>%
mutate_each(funs(wordCount)) %>%
transmute(word_diff = abs(question1-question2),
word_diff_stop = abs(question1Clean-question2clean))
wordDiff2 <- dt %>% select_(quote(-id)) %>%
mutate_each(funs(wordCount)) %>%
transmute(word_diff = abs(question1-question2),
word_diff_stop = abs(question1Clean-question2Clean))
View(wordDiff2)
identical(wordDiff, wordDiff2)
charDiff <- data %>% select(starts_with("question")) %>%
mutate_each(funs(nchar)) %>%
transmute(char_diff = abs(question1-question2), char_diff_stop = abs(question1Clean-question2Clean))
charDiff <- dt %>% select(starts_with("question")) %>%
mutate_each(funs(nchar)) %>%
transmute(char_diff = abs(question1-question2), char_diff_stop = abs(question1Clean-question2Clean))
charDiff2 <- dt %>% select_(quote(-id)) %>%
mutate_each(funs(wordCount)) %>%
transmute_(char_diff = setNames(mutate_call1, "char_diff"),
char_diff_stop = setNames(mutate_call2, "char_diff_stop"))
identical(charDiff, charDiff2)
View(charDiff2)
View(charDiff)
charDiff2 <- dt %>% select_(quote(-id)) %>%
mutate_each(funs(nchar)) %>%
transmute_(char_diff = setNames(mutate_call1, "char_diff"),
char_diff_stop = setNames(mutate_call2, "char_diff_stop"))
identical(charDiff, charDiff2)
wordDiff <- dt %>% select_(quote(-id)) %>%
mutate_each(funs(wordCount)) %>%
transmute_(word_diff = setNames(mutate_call1, "word_diff"),
word_diff_stop = setNames(mutate_call2, "word_diff_stop"))
cat("Calculating character difference", "\n")
charDiff <- dt %>% select_(quote(-id)) %>%
mutate_each(funs(nchar)) %>%
transmute_(char_diff = setNames(mutate_call1, "char_diff"),
char_diff_stop = setNames(mutate_call2, "char_diff_stop"))
data <- data.table(data)
data <- cbind(data, wordDiff, charDiff)
View(data)
View(data)
#################
# Preprocessing
#################
setwd("~/Google Drive/PNLP")
Sys.setlocale(locale = "es_ES.UTF-8") # Para visualizar caracteres especiales
library(text2vec)
library(data.table)
library(dplyr)
library(magrittr)
source("keyFunctions.R")
what <- "train"
# Leyendo datos y transformando
if (what == "train") {
df <- fread("data/train_features.csv")
} else if (what == "test") {
df <- fread("data/test_features.csv")
}
# Feature extraction ----------------------------------------------------
start.time <- Sys.time()
df <- getSizeFeatures(df, question1, question2) # Variables básica de diffLargo
df$wordShare <- wordShareIndex(df, question1, question2) # Variable de % de palabras compartidas
#################
# Preprocessing
#################
setwd("~/Google Drive/PNLP")
Sys.setlocale(locale = "es_ES.UTF-8") # Para visualizar caracteres especiales
library(text2vec)
library(data.table)
library(dplyr)
library(magrittr)
source("keyFunctions.R")
what <- "train"
# Leyendo datos y transformando
if (what == "train") {
df <- fread("data/train_features.csv")
} else if (what == "test") {
df <- fread("data/test_features.csv")
}
# Feature extraction ----------------------------------------------------
start.time <- Sys.time()
df <- getSizeFeatures(df, question1, question2, id = id) # Variables básica de diffLargo
